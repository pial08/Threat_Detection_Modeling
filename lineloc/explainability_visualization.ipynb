{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c468b22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../data/big-vul_dataset/test.csv')\n",
    "\n",
    "#df[\"flaw_line\"].notnull()\n",
    "    \n",
    "#df = df[df['flaw_line'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a266e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import argparse\n",
    "import glob\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, SequentialSampler, RandomSampler,TensorDataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from transformers import (WEIGHTS_NAME, AdamW, get_linear_schedule_with_warmup,\n",
    "                          RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer)\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "from linevul_model import Model\n",
    "import pandas as pd\n",
    "# metrics\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "from sklearn.metrics import auc\n",
    "# model reasoning\n",
    "from captum.attr import LayerIntegratedGradients, DeepLift, DeepLiftShap, GradientShap, Saliency\n",
    "# word-level tokenizer\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "# New Imports\n",
    "from operator import itemgetter\n",
    "from itertools import groupby\n",
    "import math\n",
    "\n",
    "from captum.attr import visualization as viz\n",
    "from captum.attr import LayerConductance, LayerIntegratedGradients\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9777ea26",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "## parameters\n",
    "parser.add_argument(\"--train_data_file\", default=\"../data/big-vul_dataset/train.csv\", type=str, required=False,\n",
    "                    help=\"The input training data file (a csv file).\")\n",
    "parser.add_argument(\"--output_dir\", default=\"./saved_models\", type=str, required=False,\n",
    "                    help=\"The output directory where the model predictions and checkpoints will be written.\")\n",
    "parser.add_argument(\"--model_type\", default=\"roberta\", type=str,\n",
    "                    help=\"The model architecture to be fine-tuned.\")\n",
    "parser.add_argument(\"--block_size\", default=512, type=int,\n",
    "                    help=\"Optional input sequence length after tokenization.\"\n",
    "                         \"The training dataset will be truncated in block of this size for training.\"\n",
    "                         \"Default to the model max input length for single sentence inputs (take into account special tokens).\")\n",
    "parser.add_argument(\"--eval_data_file\", default=\"../data/big-vul_dataset/val.csv\", type=str,\n",
    "                    help=\"An optional input evaluation data file to evaluate the perplexity on (a text file).\")\n",
    "parser.add_argument(\"--test_data_file\", default=\"../data/big-vul_dataset/mini_test.csv\", type=str,\n",
    "                    help=\"An optional input evaluation data file to evaluate the perplexity on (a text file).\")\n",
    "parser.add_argument(\"--model_name\", default=\"12heads_linevul_model.bin\", type=str,\n",
    "                    help=\"Saved model name.\")\n",
    "parser.add_argument(\"--model_name_or_path\", default=\"microsoft/codebert-base\", type=str,\n",
    "                    help=\"The model checkpoint for weights initialization.\")\n",
    "parser.add_argument(\"--config_name\", default=\"\", type=str,\n",
    "                    help=\"Optional pretrained config name or path if not the same as model_name_or_path\")\n",
    "parser.add_argument(\"--use_non_pretrained_model\", action='store_true', default=False,\n",
    "                    help=\"Whether to use non-pretrained model.\")\n",
    "parser.add_argument(\"--tokenizer_name\", default=\"microsoft/codebert-base\", type=str,\n",
    "                    help=\"Optional pretrained tokenizer name or path if not the same as model_name_or_path\")\n",
    "parser.add_argument(\"--code_length\", default=256, type=int,\n",
    "                    help=\"Optional Code input sequence length after tokenization.\") \n",
    "\n",
    "parser.add_argument(\"--do_train\", action='store_true',\n",
    "                    help=\"Whether to run training.\")\n",
    "parser.add_argument(\"--do_eval\", action='store_true',\n",
    "                    help=\"Whether to run eval on the dev set.\")\n",
    "parser.add_argument(\"--do_test\", action='store_true',\n",
    "                    help=\"Whether to run eval on the dev set.\")\n",
    "\n",
    "parser.add_argument(\"--evaluate_during_training\", action='store_true',\n",
    "                    help=\"Run evaluation during training at each logging step.\")\n",
    "parser.add_argument(\"--do_local_explanation\", default=False, action='store_true',\n",
    "                    help=\"Whether to do local explanation. \") \n",
    "parser.add_argument(\"--reasoning_method\", default=\"deeplift_shap\", type=str,\n",
    "                    help=\"Should be one of 'attention', 'shap', 'lime', 'lig'\")\n",
    "\n",
    "parser.add_argument(\"--train_batch_size\", default=1, type=int,\n",
    "                    help=\"Batch size per GPU/CPU for training.\")\n",
    "parser.add_argument(\"--eval_batch_size\", default=1, type=int,\n",
    "                    help=\"Batch size per GPU/CPU for evaluation.\")\n",
    "parser.add_argument('--gradient_accumulation_steps', type=int, default=1,\n",
    "                    help=\"Number of updates steps to accumulate before performing a backward/update pass.\")\n",
    "parser.add_argument(\"--learning_rate\", default=5e-5, type=float,\n",
    "                    help=\"The initial learning rate for Adam.\")\n",
    "parser.add_argument(\"--weight_decay\", default=0.0, type=float,\n",
    "                    help=\"Weight deay if we apply some.\")\n",
    "parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float,\n",
    "                    help=\"Epsilon for Adam optimizer.\")\n",
    "parser.add_argument(\"--max_grad_norm\", default=1.0, type=float,\n",
    "                    help=\"Max gradient norm.\")\n",
    "parser.add_argument(\"--max_steps\", default=-1, type=int,\n",
    "                    help=\"If > 0: set total number of training steps to perform. Override num_train_epochs.\")\n",
    "parser.add_argument(\"--warmup_steps\", default=0, type=int,\n",
    "                    help=\"Linear warmup over warmup_steps.\")\n",
    "parser.add_argument('--seed', type=int, default=42,\n",
    "                    help=\"random seed for initialization\")\n",
    "parser.add_argument('--epochs', type=int, default=1,\n",
    "                    help=\"training epochs\")\n",
    "# RQ2\n",
    "parser.add_argument(\"--effort_at_top_k\", default=0.2, type=float,\n",
    "                    help=\"Effort@TopK%Recall: effort at catching top k percent of vulnerable lines\")\n",
    "parser.add_argument(\"--top_k_recall_by_lines\", default=0.01, type=float,\n",
    "                    help=\"Recall@TopK percent, sorted by line scores\")\n",
    "parser.add_argument(\"--top_k_recall_by_pred_prob\", default=0.2, type=float,\n",
    "                    help=\"Recall@TopK percent, sorted by prediction probabilities\")\n",
    "\n",
    "parser.add_argument(\"--do_sorting_by_line_scores\", default=True, action='store_true',\n",
    "                    help=\"Whether to do sorting by line scores.\")\n",
    "parser.add_argument(\"--do_sorting_by_pred_prob\", default=False, action='store_true',\n",
    "                    help=\"Whether to do sorting by prediction probabilities.\")\n",
    "# RQ3 - line-level evaluation\n",
    "parser.add_argument('--top_k_constant', type=int, default=10,\n",
    "                    help=\"Top-K Accuracy constant\")\n",
    "# num of attention heads\n",
    "parser.add_argument('--num_attention_heads', type=int, default=12,\n",
    "                    help=\"number of attention heads used in CodeBERT\")\n",
    "# raw predictions\n",
    "parser.add_argument(\"--write_raw_preds\", default=False, action='store_true',\n",
    "                        help=\"Whether to write raw predictions on test data.\")\n",
    "# word-level tokenizer\n",
    "parser.add_argument(\"--use_word_level_tokenizer\", default=False, action='store_true',\n",
    "                    help=\"Whether to use word-level tokenizer.\")\n",
    "# bpe non-pretrained tokenizer\n",
    "parser.add_argument(\"--use_non_pretrained_tokenizer\", default=False, action='store_true',\n",
    "                    help=\"Whether to use non-pretrained bpe tokenizer.\")\n",
    "#args = parser.parse_args()\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "    # Setup CUDA, GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "args.n_gpu = torch.cuda.device_count()\n",
    "args.device = device\n",
    "    # Setup logging\n",
    "logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',datefmt='%m/%d/%Y %H:%M:%S',level=logging.INFO)\n",
    "logger.warning(\"device: %s, n_gpu: %s\",device, args.n_gpu,)\n",
    "    # Set seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba14297",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_longest_list(lst):\n",
    "    return max(lst, key=len)\n",
    "\n",
    "\n",
    "def consequent(data):\n",
    "    finalList = []\n",
    "    for k, g in groupby(enumerate(data), lambda ix : ix[0] - ix[1]):\n",
    "        #print(list(map(itemgetter(1), g)))\n",
    "        finalList.append(list(map(itemgetter(1), g)))\n",
    "     \n",
    "    return get_longest_list(finalList)\n",
    "\n",
    "\n",
    "def find(tokenList, begin, end):\n",
    "    first = -1\n",
    "    last = -1\n",
    "    for i in range(len(tokenList)):\n",
    "        if begin in tokenList[i]:\n",
    "            first = i\n",
    "        elif end in tokenList[i]:\n",
    "            last = i\n",
    "\n",
    "        if first != -1 and last != -1:\n",
    "            return (first, last)\n",
    "    return (0, 0)\n",
    "\n",
    "def calculateIoU(locLogits, lines):\n",
    "    \n",
    "    IoU = 0\n",
    "    counter = 0\n",
    "    IoUList = []\n",
    "    for i in range(len(locLogits)):\n",
    "        logitSet = []\n",
    "        lineSet = []\n",
    "        \n",
    "        logitSet = set(range(int(locLogits[i][0]), int(locLogits[i][1])))\n",
    "        lineSet = set(range(lines[i][0], lines[i][1]))\n",
    "\n",
    "        if len(logitSet | lineSet) == 0:\n",
    "            continue\n",
    "        counter += 1\n",
    "        #print(\"Calculating IoU: \", locLogits, lines)\n",
    "        #print(\"IoU: \", len(logitSet & lineSet), len(logitSet | lineSet))\n",
    "        IoUList.append(len(logitSet & lineSet) / len(logitSet | lineSet))\n",
    "        IoU += len(logitSet & lineSet) / len(logitSet | lineSet) \n",
    "    \n",
    "    print(\"IoUs ...\", IoUList)\n",
    "    return (IoU / counter), IoUList\n",
    "\n",
    "\n",
    "class InputFeatures(object):\n",
    "    \"\"\"A single training/test features for a example.\"\"\"\n",
    "    def __init__(self,\n",
    "                 input_tokens,\n",
    "                 input_ids,\n",
    "                 label, \n",
    "                 line):\n",
    "        self.input_tokens = input_tokens\n",
    "        self.input_ids = input_ids\n",
    "        self.label=label\n",
    "        self.line=line\n",
    "        \n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, tokenizer, args, file_type=\"train\"):\n",
    "        if file_type == \"train\":\n",
    "            file_path = args.train_data_file\n",
    "        elif file_type == \"eval\":\n",
    "            file_path = args.eval_data_file\n",
    "        elif file_type == \"test\":\n",
    "            file_path = args.test_data_file\n",
    "        self.examples = []\n",
    "        df = pd.read_csv(file_path)\n",
    "        #df = df.head(1000)\n",
    "        \n",
    "        new_tokens = [\"cat\", \"dog\"]\n",
    "\n",
    "        # check if the tokens are already in the vocabulary\n",
    "        #new_tokens = set(new_tokens) - set(tokenizer.vocab.keys())\n",
    "        #print(\"Printing new tokens ...\", new_tokens)\n",
    "        # add the tokens to the tokenizer vocabulary\n",
    "        tokenizer.add_tokens(list(new_tokens))\n",
    "\n",
    "        # Line added by Nafis\n",
    "        #df = df.head(1000)\n",
    "        \n",
    "        df.fillna('', inplace=True)\n",
    "\n",
    "        funcs = df[\"processed_func\"].tolist()\n",
    "        labels = df[\"target\"].tolist()\n",
    "\n",
    "        #print()\n",
    "        flaw_line_index = df[\"flaw_line_index\"].tolist()\n",
    "    \n",
    "        #print(flaw_line_index)\n",
    "        for i in tqdm(range(len(funcs))):\n",
    "            #print(\"Printing functions ...........\", funcs[i])\n",
    "            #print(\"type of index ... \", type(flaw_line_index[i]))\n",
    "            \n",
    "            # if the value is NaN(float)\n",
    "            #print(\"label ...\", labels[i])\n",
    "\n",
    "            #*********************************************************************************#\n",
    "            ############### **** Check if Both Vuln and Non-Vuln Passes **** ##################\n",
    "            #*********************************************************************************#\n",
    "\n",
    "            if labels[i] == 1 and flaw_line_index[i] != \"\":\n",
    "                #continue\n",
    "                self.examples.append(convert_examples_to_features(funcs[i], labels[i], flaw_line_index[i], tokenizer, args, file_type))\n",
    "        if file_type == \"train\":\n",
    "            for example in self.examples[:3]:\n",
    "                    logger.info(\"*** Example ***\")\n",
    "                    logger.info(\"label: {}\".format(example.label))\n",
    "                    logger.info(\"input_tokens: {}\".format([x.replace('\\u0120','_') for x in example.input_tokens]))\n",
    "                    logger.info(\"input_ids: {}\".format(' '.join(map(str, example.input_ids))))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, i):       \n",
    "        return torch.tensor(self.examples[i].input_ids),torch.tensor(self.examples[i].label), torch.tensor(self.examples[i].line)\n",
    "\n",
    "\n",
    "def convert_examples_to_features(func, label, flaw_line, tokenizer, args, file_type):\n",
    "    #print(\"Word level tokenizer\", args.use_word_level_tokenizer)\n",
    "    if args.use_word_level_tokenizer:\n",
    "        encoded = tokenizer.encode(func)\n",
    "        encoded = encoded.ids\n",
    "        if len(encoded) > 510:\n",
    "            encoded = encoded[:510]\n",
    "        encoded.insert(0, 0)\n",
    "        encoded.append(2)\n",
    "        if len(encoded) < 512:\n",
    "            padding = 512 - len(encoded)\n",
    "            for _ in range(padding):\n",
    "                encoded.append(1)\n",
    "        source_ids = encoded\n",
    "        source_tokens = []\n",
    "        return InputFeatures(source_tokens, source_ids, label)\n",
    "    # source\n",
    "\n",
    "    \n",
    "    begin = \"cat\"\n",
    "    end = \"dog\"\n",
    "    if label == 1:\n",
    "        #print(\"Type of line ... \", type(flaw_line))\n",
    "        if isinstance(flaw_line, int):\n",
    "            #print(\"Type truer\")\n",
    "            flaw_line = \"\".join(str(flaw_line))\n",
    "        flaw_line = flaw_line.split(\",\")\n",
    "        flaw_line = [int(i) for i in flaw_line]\n",
    "        #print(\"flaw fline \", flaw_line, type(flaw_line))\n",
    "        consequentVulLines = consequent(flaw_line)\n",
    "        #print(\"Flaw line ...\", consequentVulLines)\n",
    "        #print(\"Original function ... \", func)\n",
    "        first, last =  consequentVulLines[0] - 1, consequentVulLines[-1] - 1\n",
    "        #print(\"First and last...\", first, last)\n",
    "        funcLines = func.split(\"\\n\")\n",
    "        func = \"\"\n",
    "        boolBegin = True\n",
    "        boolEnd = True\n",
    "        for i in range(len(funcLines)):\n",
    "            if i == first and first == last and boolBegin and boolEnd:\n",
    "                #print(\"Odd case\", funcLines[i])\n",
    "                func += begin + funcLines[i] + end + \"\\n\"\n",
    "                boolBegin = False\n",
    "                boolEnd = False\n",
    "                #break\n",
    "            if i == first and boolBegin:\n",
    "                func += begin + funcLines[i] + \"\\n\"\n",
    "                boolBegin = True\n",
    "            elif i == last and boolEnd:\n",
    "                func += funcLines[i] + end + \"\\n\"\n",
    "                boolEnd = False\n",
    "            else:\n",
    "                func += funcLines[i] + \" \\n \"\n",
    "        \n",
    "        #print(\"Final Function ... \", func)\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    #print(tokenizer.mask_token)\n",
    "    code_tokens = tokenizer.tokenize(str(func))[:args.block_size-2]\n",
    "    lines = find(code_tokens, begin, end) \n",
    "\n",
    "    if file_type == \"test\" or file_type == \"eval\":\n",
    "        func = func.replace(\"cat\", \"\") \n",
    "        func = func.replace(\"dog\", \"\") \n",
    "        code_tokens = tokenizer.tokenize(str(func))[:args.block_size-2] \n",
    "\n",
    "    #print(\"Printing Vulnerable lines ...........\", lines)\n",
    "    \"\"\"\n",
    "    if code_tokens.count(begin) == 1 and code_tokens.count(end) == 1:\n",
    "        print(code_tokens)\n",
    "        lines = (code_tokens.index(begin), code_tokens.index(end))\n",
    "        print(\"Printing Vulnerable lines ...........\", lines)\n",
    "\n",
    "    else:\n",
    "        lines = (0, 0)    \n",
    "    \"\"\"\n",
    "    #if label == 1:\n",
    "    #    print(code_tokens)\n",
    "    #    print(code_tokens.index(begin), code_tokens.index(end))\n",
    "    \n",
    "\n",
    "    \n",
    "    source_tokens = [tokenizer.cls_token] + code_tokens + [tokenizer.sep_token]\n",
    "    source_ids = tokenizer.convert_tokens_to_ids(source_tokens)\n",
    "    padding_length = args.block_size - len(source_ids)\n",
    "    source_ids += [tokenizer.pad_token_id] * padding_length\n",
    "    return InputFeatures(source_tokens, source_ids, label, lines)\n",
    "\n",
    "def set_seed(args):\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(args.seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47756096",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(args, model, tokenizer, test_dataset, best_threshold=0.5):\n",
    "    # build dataloader\n",
    "    test_sampler = SequentialSampler(test_dataset)\n",
    "    test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=args.eval_batch_size, num_workers=0)\n",
    "    \n",
    "    \n",
    "    # multi-gpu evaluate\n",
    "    if args.n_gpu > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    # Eval!\n",
    "    \n",
    "    logger.info(\"***** Running Test *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(test_dataset))\n",
    "    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    model.eval()\n",
    "    logits=[]  \n",
    "    y_trues=[]\n",
    "\n",
    "    # For line level evaluation: Nafis\n",
    "    y_lines_true = []\n",
    "    locLogits = []\n",
    "\n",
    "    for batch in test_dataloader:\n",
    "        (inputs_ids, labels, lines) = [x.to(args.device) for x in batch]\n",
    "        with torch.no_grad():\n",
    "            lm_loss, logit, localizerLogits = model(input_ids=inputs_ids, labels=labels, lines=lines)\n",
    "            eval_loss += lm_loss.mean().item()\n",
    "            logits.append(logit.cpu().numpy())\n",
    "            locLogits.append(localizerLogits.cpu().numpy())\n",
    "            y_trues.append(labels.cpu().numpy())\n",
    "            y_lines_true.append(lines.cpu().numpy())\n",
    "        nb_eval_steps += 1\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    # calculate scores\n",
    "    logits = np.concatenate(logits, 0)\n",
    "    y_trues = np.concatenate(y_trues, 0)\n",
    "    locLogits = np.concatenate(locLogits, 0)\n",
    "    y_lines_true = np.concatenate(y_lines_true, 0)\n",
    "    y_preds = logits[:, 1] > best_threshold\n",
    "    acc = accuracy_score(y_trues, y_preds)\n",
    "    recall = recall_score(y_trues, y_preds)\n",
    "    precision = precision_score(y_trues, y_preds)   \n",
    "    f1 = f1_score(y_trues, y_preds)  \n",
    "    IoU, IoUList = calculateIoU(locLogits, y_lines_true)            \n",
    "    result = {\n",
    "        \"test_accuracy\": float(acc),\n",
    "        \"test_recall\": float(recall),\n",
    "        \"test_precision\": float(precision),\n",
    "        \"test_f1\": float(f1),\n",
    "        \"test_threshold\":best_threshold,\n",
    "        \"IoU\": IoU,\n",
    "    }\n",
    "\n",
    "    logger.info(\"***** Test results *****\")\n",
    "    for key in sorted(result.keys()):\n",
    "        logger.info(\"  %s = %s\", key, str(round(result[key],4)))\n",
    "\n",
    "    logits = [l[1] for l in logits]\n",
    "    result_df = generate_result_df(logits, y_trues, y_preds, IoUList, args)\n",
    "    sum_lines, sum_flaw_lines = get_line_statistics(result_df)\n",
    "\n",
    "    result_df = result_df.sort_values(by=['IoU'], ascending=False)\n",
    "    \n",
    "    #print(\"Printing Result DF ............\", result_df)\n",
    "    # write raw predictions if needed\n",
    "    if args.write_raw_preds:\n",
    "        write_raw_preds_csv(args, y_preds)\n",
    "\n",
    "    result_df.to_csv('result_df.csv')  \n",
    "    \n",
    "\n",
    "    #result_df = pd.read_csv('result_df.csv')\n",
    "\n",
    "    # define reasoning method\n",
    "    if args.reasoning_method == \"all\":\n",
    "            all_reasoning_method = [\"attention\", \"deeplift_shap\", \"saliency\", \"lig\", \"deeplift\", \"gradient_shap\"]\n",
    "    else:\n",
    "        all_reasoning_method = [args.reasoning_method]\n",
    "\n",
    "    if args.do_sorting_by_line_scores:\n",
    "        # (RQ2) Effort@TopK%Recall & Recall@TopK%LOC for the whole test set\n",
    "        # flatten the logits\n",
    "        print(\"Sorting by line scores\")\n",
    "        for reasoning_method in all_reasoning_method:\n",
    "            dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=1, num_workers=0)\n",
    "            # Updated by Nafis\n",
    "            #dataloader = test_dataloader\n",
    "            #dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=8, num_workers=0)\n",
    "\n",
    "            progress_bar = tqdm(dataloader, total=len(dataloader))\n",
    "            all_pos_score_label = []\n",
    "            all_neg_score_label = []\n",
    "            index = 0\n",
    "            total_pred_as_vul = 0\n",
    "            print(\"CK-1\")\n",
    "            for mini_batch in progress_bar:\n",
    "                print(\"CK-2\")\n",
    "                # if predicted as vulnerable\n",
    "                if result_df[\"logits\"][index] > 0.5 or True:\n",
    "                    print(\"Buggy Line\")\n",
    "                    total_pred_as_vul += 1\n",
    "                    all_lines_score_with_label = \\\n",
    "                    line_level_localization(flaw_lines=result_df[\"flaw_line\"][index],\n",
    "                                            tokenizer=tokenizer, \n",
    "                                            model=model, \n",
    "                                            mini_batch=mini_batch, \n",
    "                                            original_func=result_df[\"processed_func\"][index], \n",
    "                                            args=args,\n",
    "                                            top_k_loc=None,\n",
    "                                            top_k_constant=None,\n",
    "                                            reasoning_method=reasoning_method,\n",
    "                                            index=index)\n",
    "                    all_pos_score_label.append(all_lines_score_with_label)\n",
    "                # else predicted as non vulnerable\n",
    "                \"\"\"\n",
    "                else:\n",
    "                    print(\"Benign Line\")\n",
    "                    all_lines_score_with_label = \\\n",
    "                    line_level_localization(flaw_lines=result_df[\"flaw_line\"][index],\n",
    "                                            tokenizer=tokenizer, \n",
    "                                            model=model, \n",
    "                                            mini_batch=mini_batch, \n",
    "                                            original_func=result_df[\"processed_func\"][index], \n",
    "                                            args=args,\n",
    "                                            top_k_loc=None,\n",
    "                                            top_k_constant=None,\n",
    "                                            reasoning_method=reasoning_method,\n",
    "                                            index=index)\n",
    "                    all_neg_score_label.append(all_lines_score_with_label)\n",
    "                \"\"\"\n",
    "                index += 1\n",
    "            is_attention = True if reasoning_method == \"attention\" else False            \n",
    "            total_pos_lines, pos_rank_df  = rank_lines(all_pos_score_label, is_attention, ascending_ranking=False)\n",
    "            \n",
    "            if is_attention:\n",
    "                total_neg_lines, neg_rank_df  = rank_lines(all_neg_score_label, is_attention, ascending_ranking=True)\n",
    "            else:\n",
    "                total_neg_lines, neg_rank_df  = rank_lines(all_neg_score_label, is_attention, ascending_ranking=False)\n",
    "            \n",
    "            effort, inspected_line = top_k_effort(pos_rank_df, sum_lines, sum_flaw_lines, args.effort_at_top_k)\n",
    "\n",
    "            recall_value = top_k_recall(pos_rank_df, neg_rank_df, sum_lines, sum_flaw_lines, args.top_k_recall_by_lines)\n",
    "\n",
    "            logger.info(f\"total functions predicted as vulnerable: {total_pred_as_vul}\")\n",
    "\n",
    "            to_write = \"\"\n",
    "\n",
    "            to_write += \"\\n\" + f\"Reasoning Method: {reasoning_method}\" + \"\\n\"\n",
    "\n",
    "            to_write += f\"total predicted vulnerable lines: {total_pos_lines}\" + \"\\n\"\n",
    "            logger.info(f\"total predicted vulnerable lines: {total_pos_lines}\")\n",
    "\n",
    "            to_write += f\"total lines: {sum_lines}\" + \"\\n\"\n",
    "            logger.info(f\"total lines: {sum_lines}\")\n",
    "            \n",
    "            to_write += f\"total flaw lines: {sum_flaw_lines}\" + \"\\n\"\n",
    "            logger.info(f\"total flaw lines: {sum_flaw_lines}\")\n",
    "            \n",
    "            vul_as_vul = sum(pos_rank_df[\"label\"].tolist())\n",
    "            to_write += f\"total flaw lines in predicted as vulnerable: {vul_as_vul}\" + \"\\n\"\n",
    "            logger.info(f\"total flaw lines in predicted as vulnerable: {vul_as_vul}\")\n",
    "            \n",
    "            to_write += f\"top{args.effort_at_top_k}-Effort: {effort}\" + \"\\n\"\n",
    "            logger.info(f\"top{args.effort_at_top_k}-Effort: {effort}\")\n",
    "            \n",
    "            to_write += f\"total inspected line to find out {args.effort_at_top_k} of flaw lines: {inspected_line}\" + \"\\n\"\n",
    "            logger.info(f\"total inspected line to find out {args.effort_at_top_k} of flaw lines: {inspected_line}\")\n",
    "            \n",
    "            to_write += f\"top{args.top_k_recall_by_lines}-Recall: {recall_value}\" + \"\\n\"\n",
    "            logger.info(f\"top{args.top_k_recall_by_lines}-Recall: {recall_value}\")\n",
    "            \n",
    "            with open(\"./results/rq2_result.txt\", \"a\") as f:\n",
    "                f.write(to_write)\n",
    "\n",
    "    if args.do_sorting_by_pred_prob:\n",
    "        rank_df = rank_dataframe(df=result_df, rank_by=\"logits\", ascending=False)\n",
    "        effort, inspected_line = top_k_effort_pred_prob(rank_df, sum_lines, sum_flaw_lines, args.effort_at_top_k, label_col_name=\"y_preds\")\n",
    "        top_k_recall_val = top_k_recall_pred_prob(rank_df, sum_lines, sum_flaw_lines, args.top_k_recall_by_pred_prob, label_col_name=\"y_preds\")\n",
    "        with open(\"./results/rq2_result_pred_prob.txt\", \"a\") as f:\n",
    "            f.write(f\"\\n Sorted By Prediction Probabilities \\n top{args.effort_at_top_k}-Effort: {effort} \\n top{args.top_k_recall_by_pred_prob}-Recall: {top_k_recall_val}\")\n",
    "            logger.info(f\"\\n Sorted By Prediction Probabilities \\n top{args.effort_at_top_k}-Effort: {effort} \\n top{args.top_k_recall_by_pred_prob}-Recall: {top_k_recall_val}\")\n",
    "\n",
    "    # (RQ3) Line level evaluation for True Positive cases\n",
    "    if args.do_local_explanation:\n",
    "        for reasoning_method in all_reasoning_method:\n",
    "            logger.info(f\"***** Running Explanation - {reasoning_method} *****\")\n",
    "            correct_indices = np.where((y_trues == y_preds))\n",
    "            correct_indices = list(correct_indices[0])\n",
    "            print(\"correct prediction count: \", len(correct_indices))\n",
    "            tp_indices = np.where((y_trues == y_preds) & (y_trues == 1))\n",
    "            tp_indices = list(tp_indices[0])\n",
    "            print(\"correct vulnerable count: \", len(tp_indices))\n",
    "            # localization part\n",
    "            dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=1, num_workers=0)\n",
    "            # prepare data for line-level reasoning\n",
    "            df = pd.read_csv(args.test_data_file)\n",
    "            # stats for line-level evaluation\n",
    "            top_k_locs = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "            top_k_constant = [args.top_k_constant]\n",
    "            sum_total_lines = 0\n",
    "            total_flaw_lines = 0\n",
    "            total_function = 0\n",
    "            all_top_10_correct_idx = []\n",
    "            all_top_10_not_correct_idx = []\n",
    "            # for CodeBERT reasoning\n",
    "            total_correctly_predicted_flaw_lines = [0 for _ in range(len(top_k_locs))]\n",
    "            total_correctly_localized_function = [0 for _ in range(len(top_k_constant))]\n",
    "            total_min_clean_lines_inspected = 0\n",
    "            ifa_records = []\n",
    "            top_10_acc_records = []\n",
    "            total_max_clean_lines_inspected = 0\n",
    "            # vulnerability exist but not applicable (flaw tokens are out of seq length)\n",
    "            na_explanation_total = 0\n",
    "            na_eval_results_512 = 0\n",
    "            na_defective_data_point = 0\n",
    "            # track progress\n",
    "            progress_bar = tqdm(dataloader, total=len(dataloader))\n",
    "            # used to locate the row in test data\n",
    "            index = 0\n",
    "            for mini_batch in progress_bar:\n",
    "                # if true positive (vulnerable predicted as vulnerable), do explanation\n",
    "                if index in tp_indices:\n",
    "                    # if flaw line exists\n",
    "                    # if not exist, the data is as type of float (nan)\n",
    "                    if isinstance(df[\"flaw_line\"][index], str) and isinstance(df[\"flaw_line_index\"][index], str):                        \n",
    "                        line_eval_results = \\\n",
    "                        line_level_localization_tp(flaw_lines=df[\"flaw_line\"][index],\n",
    "                                                tokenizer=tokenizer, \n",
    "                                                model=model, \n",
    "                                                mini_batch=mini_batch, \n",
    "                                                original_func=df[\"processed_func\"][index], \n",
    "                                                args=args,\n",
    "                                                top_k_loc=top_k_locs,\n",
    "                                                top_k_constant=top_k_constant,\n",
    "                                                reasoning_method=reasoning_method,\n",
    "                                                index=index,\n",
    "                                                write_invalid_data=False)\n",
    "                        if line_eval_results == \"NA\":\n",
    "                            na_explanation_total += 1 \n",
    "                            na_eval_results_512 += 1\n",
    "                        else:                       \n",
    "                            total_function += 1\n",
    "                            sum_total_lines += line_eval_results[\"total_lines\"]\n",
    "                            total_flaw_lines += line_eval_results[\"num_of_flaw_lines\"]\n",
    "                            # IFA metric\n",
    "                            total_min_clean_lines_inspected += line_eval_results[\"min_clean_lines_inspected\"]\n",
    "                            \n",
    "                            # For IFA Boxplot\n",
    "                            ifa_records.append(line_eval_results[\"min_clean_lines_inspected\"])\n",
    "                            \n",
    "                            # For Top-10 Acc Boxplot\n",
    "                            # todo\n",
    "                            #top_10_acc_records.append(line_eval_results[]) \n",
    "                            \n",
    "                            # All effort metric\n",
    "                            total_max_clean_lines_inspected += line_eval_results[\"max_clean_lines_inspected\"]\n",
    "                            for j in range(len(top_k_locs)):\n",
    "                                total_correctly_predicted_flaw_lines[j] += line_eval_results[\"all_correctly_predicted_flaw_lines\"][j]\n",
    "                            # top 10 accuracy\n",
    "                            for k in range(len(top_k_constant)):\n",
    "                                total_correctly_localized_function[k] += line_eval_results[\"all_correctly_localized_function\"][k]\n",
    "                            # top 10 correct idx and not correct idx\n",
    "                            if line_eval_results[\"top_10_correct_idx\"] != []:\n",
    "                                all_top_10_correct_idx.append(line_eval_results[\"top_10_correct_idx\"][0])\n",
    "                            if line_eval_results[\"top_10_not_correct_idx\"] != []:\n",
    "                                all_top_10_not_correct_idx.append(line_eval_results[\"top_10_not_correct_idx\"][0]) \n",
    "                    else:\n",
    "                        na_explanation_total += 1\n",
    "                        na_defective_data_point += 1\n",
    "                index += 1\n",
    "\n",
    "            # write IFA records for IFA Boxplot\n",
    "            with open(f\"./ifa_records/ifa_{reasoning_method}.txt\", \"w+\") as f:\n",
    "                f.write(str(ifa_records))\n",
    "            # write Top-10 Acc records for Top-10 Accuracy Boxplot\n",
    "            # todo\n",
    "            #with open(f\"./top_10_acc_records/top_10_acc_{reasoning_method}.txt\", \"w+\") as f:\n",
    "            #    f.write(str())\n",
    "\n",
    "            logger.info(f\"Total number of functions: {total_function}\")\n",
    "            logger.info(f\"Total number of lines: {sum_total_lines}\")\n",
    "            logger.info(f\"Total number of flaw lines: {total_flaw_lines}\")\n",
    "            logger.info(f\"Total Explanation Not Applicable: {na_explanation_total}\")\n",
    "            logger.info(f\"NA Eval Results (Out of 512 Tokens): {na_eval_results_512}\")\n",
    "            logger.info(f\"NA Defective Data Point: {na_defective_data_point}\")\n",
    "\n",
    "            line_level_results = [{f\"codebert_{reasoning_method}_top20%_recall\": \n",
    "                                [round(total_correctly_predicted_flaw_lines[i] / total_flaw_lines, 2) * 100 for i in range(len(top_k_locs))],\n",
    "                                f\"codebert_{reasoning_method}_top10_accuracy\":\n",
    "                                [round(total_correctly_localized_function[i] / total_function, 2) * 100 for i in range(len(top_k_constant))],\n",
    "                                f\"codebert_{reasoning_method}_ifa\": \n",
    "                                round(total_min_clean_lines_inspected / total_function, 2),\n",
    "                                f\"codebert_{reasoning_method}_recall@topk%loc_auc\":\n",
    "                                auc(x=top_k_locs, y=[round(total_correctly_predicted_flaw_lines[i] / total_flaw_lines, 2) for i in range(len(top_k_locs))]),\n",
    "                                f\"codebert_{reasoning_method}_total_effort\":\n",
    "                                round(total_max_clean_lines_inspected / sum_total_lines, 2),\n",
    "                                \"avg_line_in_one_func\": \n",
    "                                int(sum_total_lines / total_function),\n",
    "                                \"total_func\": \n",
    "                                total_function,\n",
    "                                \"all_top_10_correct_idx\": all_top_10_correct_idx,\n",
    "                                \"all_top_10_not_correct_idx\": all_top_10_not_correct_idx}]\n",
    "                                \n",
    "            with open('./results/line_level_correct_idx.pkl', 'wb') as f:\n",
    "                pickle.dump(all_top_10_correct_idx, f)\n",
    "            with open('./results/line_level_not_correct_idx.pkl', 'wb') as f:\n",
    "                pickle.dump(all_top_10_not_correct_idx, f)\n",
    "\n",
    "            logger.info(\"***** Line Level Result *****\")\n",
    "            logger.info(line_level_results)\n",
    "            # output results\n",
    "            # with open(\"./results/local_explanation.pkl\", \"wb\") as f:\n",
    "            #    pickle.dump(line_level_results, f)\n",
    "\n",
    "def generate_result_df(logits, y_trues, y_preds, IoUList, args):\n",
    "    df = pd.read_csv(args.test_data_file)\n",
    "    all_num_lines = []\n",
    "    all_processed_func = df[\"processed_func\"].tolist()\n",
    "\n",
    "    print(\"Len of original DF -------\", len(df))\n",
    "\n",
    "    for func in all_processed_func:\n",
    "        all_num_lines.append(get_num_lines(func))\n",
    "    flaw_line_indices = df[\"flaw_line_index\"].tolist()\n",
    "    all_num_flaw_lines = []\n",
    "    total_flaw_lines = 0\n",
    "    for indices in flaw_line_indices:\n",
    "        if isinstance(indices, str):\n",
    "            indices = indices.split(\",\")\n",
    "            num_flaw_lines = len(indices)\n",
    "            total_flaw_lines += num_flaw_lines\n",
    "        else:\n",
    "            num_flaw_lines = 0\n",
    "        all_num_flaw_lines.append(num_flaw_lines)\n",
    "\n",
    "    print(\"Printing all lens (logits, y_trues, y_preds, all_num_flaw_lines)...\",len(logits), len(y_trues), len(y_preds), len(all_num_flaw_lines))\n",
    "    assert len(logits) == len(y_trues) == len(y_preds) == len(all_num_flaw_lines)\n",
    "    \n",
    "    return pd.DataFrame({\"logits\": logits, \"y_trues\": y_trues, \"y_preds\": y_preds, \n",
    "                         \"index\": list(range(len(logits))), \"num_flaw_lines\": all_num_flaw_lines, \"num_lines\": all_num_lines, \n",
    "                         \"flaw_line\": df[\"flaw_line\"], \"processed_func\": df[\"processed_func\"], \"IoU\":IoUList})\n",
    "\n",
    "\n",
    "    \n",
    "def write_raw_preds_csv(args, y_preds):\n",
    "    df = pd.read_csv(args.test_data_file)\n",
    "    df[\"raw_preds\"] = y_preds\n",
    "    df.to_csv(\"./results/raw_preds.csv\", index=False)\n",
    "\n",
    "def get_num_lines(func):\n",
    "    func = func.split(\"\\n\")\n",
    "    func = [line for line in func if len(line) > 0]\n",
    "    return len(func)\n",
    "\n",
    "def get_line_statistics(result_df):\n",
    "    total_lines = sum(result_df[\"num_lines\"].tolist())\n",
    "    total_flaw_lines = sum(result_df[\"num_flaw_lines\"].tolist())\n",
    "    return total_lines, total_flaw_lines\n",
    "\n",
    "def rank_lines(all_lines_score_with_label, is_attention, ascending_ranking):\n",
    "    # flatten the list\n",
    "    all_lines_score_with_label = [line for lines in all_lines_score_with_label for line in lines]\n",
    "    if is_attention:\n",
    "        all_scores = [line[0].item() for line in all_lines_score_with_label]\n",
    "    else:\n",
    "        all_scores = [line[0] for line in all_lines_score_with_label]\n",
    "    all_labels = [line[1] for line in all_lines_score_with_label]\n",
    "    rank_df = pd.DataFrame({\"score\": all_scores, \"label\": all_labels})\n",
    "    rank_df = rank_dataframe(rank_df, \"score\", ascending_ranking)\n",
    "    return len(rank_df), rank_df\n",
    "\n",
    "def rank_dataframe(df, rank_by: str, ascending: bool):\n",
    "    df = df.sort_values(by=[rank_by], ascending=ascending)\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def top_k_effort(rank_df, sum_lines, sum_flaw_lines, top_k_loc, label_col_name=\"label\"):\n",
    "    target_flaw_line = int(sum_flaw_lines * top_k_loc)\n",
    "    caught_flaw_line = 0\n",
    "    inspected_line = 0\n",
    "    for i in range(len(rank_df)):\n",
    "        inspected_line += 1\n",
    "        if rank_df[label_col_name][i] == 1:\n",
    "            caught_flaw_line += 1\n",
    "        if target_flaw_line == caught_flaw_line:\n",
    "            break\n",
    "    effort = round(inspected_line / sum_lines, 4)\n",
    "    return effort, inspected_line\n",
    "\n",
    "def top_k_effort_pred_prob(rank_df, sum_lines, sum_flaw_lines, top_k_loc, label_col_name=\"y_preds\"):\n",
    "    target_flaw_line = int(sum_flaw_lines * top_k_loc)\n",
    "    caught_flaw_line = 0\n",
    "    inspected_line = 0\n",
    "    for i in range(len(rank_df)):\n",
    "        inspected_line += rank_df[\"num_lines\"][i]\n",
    "        if rank_df[label_col_name][i] == 1 or rank_df[label_col_name][i] is True:\n",
    "            caught_flaw_line += rank_df[\"num_flaw_lines\"][i]\n",
    "        if caught_flaw_line >= target_flaw_line:\n",
    "            break\n",
    "    effort = round(inspected_line / sum_lines, 4)\n",
    "    return effort, inspected_line\n",
    "\n",
    "def top_k_recall(pos_rank_df, neg_rank_df, sum_lines, sum_flaw_lines, top_k_loc):\n",
    "    target_inspected_line = int(sum_lines * top_k_loc)\n",
    "    caught_flaw_line = 0\n",
    "    inspected_line = 0\n",
    "    inspect_neg_lines = True\n",
    "    for i in range(len(pos_rank_df)):\n",
    "        inspected_line += 1\n",
    "        if inspected_line > target_inspected_line:\n",
    "            inspect_neg_lines = False\n",
    "            break\n",
    "        if pos_rank_df[\"label\"][i] == 1 or pos_rank_df[\"label\"][i] is True:\n",
    "            caught_flaw_line += 1\n",
    "    if inspect_neg_lines:\n",
    "        for i in range(len(neg_rank_df)):\n",
    "            inspected_line += 1\n",
    "            if inspected_line > target_inspected_line:\n",
    "                break\n",
    "            if neg_rank_df[\"label\"][i] == 1 or neg_rank_df[\"label\"][i] is True:\n",
    "                caught_flaw_line += 1\n",
    "    return round(caught_flaw_line / sum_flaw_lines, 4)\n",
    "\n",
    "def top_k_recall_pred_prob(rank_df, sum_lines: int, sum_flaw_lines: int, top_k_loc: float, label_col_name=\"y_preds\"):\n",
    "    target_inspected_line = int(sum_lines * top_k_loc)\n",
    "    caught_flaw_line = 0\n",
    "    inspected_line = 0\n",
    "    for i in range(len(rank_df)):\n",
    "        inspected_line += rank_df[\"num_lines\"][i]\n",
    "        if inspected_line > target_inspected_line:\n",
    "            break\n",
    "        if rank_df[label_col_name][i] == 1 or rank_df[label_col_name][i] is True:\n",
    "            caught_flaw_line += rank_df[\"num_flaw_lines\"][i]\n",
    "    return round(caught_flaw_line / sum_flaw_lines, 4)\n",
    "\n",
    "def summarize_attributions(attributions):\n",
    "    attributions = attributions.sum(dim=-1).squeeze(0)\n",
    "    attributions = attributions / torch.norm(attributions)\n",
    "    return attributions\n",
    "\n",
    "def create_ref_input_ids(input_ids, ref_token_id, sep_token_id, cls_token_id):\n",
    "    seq_length = input_ids.size(1)\n",
    "    ref_input_ids = [cls_token_id] + [ref_token_id] * (seq_length-2) + [sep_token_id]\n",
    "    return torch.tensor([ref_input_ids])\n",
    "\n",
    "def line_level_localization_tp(flaw_lines: str, tokenizer, model, mini_batch, original_func: str, args, top_k_loc: list, top_k_constant: list, reasoning_method: str, index: int, write_invalid_data: bool):\n",
    "    # function for captum LIG.\n",
    "    def predict(input_ids):\n",
    "        return model(input_ids=input_ids)[0]\n",
    "\n",
    "    def lig_forward(input_ids):\n",
    "        logits = model(input_ids=input_ids)[0]\n",
    "        y_pred = 1 # for positive attribution, y_pred = 0 for negative attribution\n",
    "        pred_prob = logits[y_pred].unsqueeze(-1)\n",
    "        return pred_prob\n",
    "\n",
    "    flaw_line_seperator = \"/~/\"\n",
    "    (input_ids, labels) = mini_batch\n",
    "    ids = input_ids[0].detach().tolist()\n",
    "    all_tokens = tokenizer.convert_ids_to_tokens(ids)\n",
    "    all_tokens = [token.replace(\"Ġ\", \"\") for token in all_tokens]\n",
    "    all_tokens = [token.replace(\"ĉ\", \"Ċ\") for token in all_tokens]\n",
    "    original_lines = ''.join(all_tokens).split(\"Ċ\")\n",
    "\n",
    "    # flaw line verification\n",
    "    # get flaw tokens ground truth\n",
    "    flaw_lines = get_all_flaw_lines(flaw_lines=flaw_lines, flaw_line_seperator=flaw_line_seperator)\n",
    "    flaw_tokens_encoded = encode_all_lines(all_lines=flaw_lines, tokenizer=tokenizer)\n",
    "    verified_flaw_lines = []\n",
    "    do_explanation = False\n",
    "    for i in range(len(flaw_tokens_encoded)):\n",
    "        encoded_flaw = ''.join(flaw_tokens_encoded[i])\n",
    "        encoded_all = ''.join(all_tokens)\n",
    "        if encoded_flaw in encoded_all:\n",
    "            verified_flaw_lines.append(flaw_tokens_encoded[i])\n",
    "            do_explanation = True\n",
    "\n",
    "    # do explanation if at least one flaw line exist in the encoded input\n",
    "    if do_explanation:\n",
    "        if reasoning_method == \"attention\":\n",
    "            # attentions: a tuple with of one Tensor with 4D shape (batch_size, num_heads, sequence_length, sequence_length)\n",
    "            input_ids = input_ids.to(args.device)\n",
    "            prob, attentions = model(input_ids=input_ids, output_attentions=True)\n",
    "            # take from tuple then take out mini-batch attention values\n",
    "            attentions = attentions[0][0]\n",
    "            #print(\"Shape of attention\", attentions.shape)\n",
    "            attention = None\n",
    "            # go into the layer\n",
    "            for i in range(len(attentions)):\n",
    "                layer_attention = attentions[i]\n",
    "                # summerize the values of each token dot other tokens\n",
    "                layer_attention = sum(layer_attention)\n",
    "                if attention is None:\n",
    "                    attention = layer_attention\n",
    "                else:\n",
    "                    attention += layer_attention\n",
    "            # clean att score for <s> and </s>\n",
    "            attention = clean_special_token_values(attention, padding=True)\n",
    "            # attention should be 1D tensor with seq length representing each token's attention value\n",
    "            word_att_scores = get_word_att_scores(all_tokens=all_tokens, att_scores=attention)\n",
    "            #print(\"Word attention scores ...\", word_att_scores)\n",
    "            all_lines_score, flaw_line_indices = get_all_lines_score(word_att_scores, verified_flaw_lines)\n",
    "            #print(\"all lines score\", all_lines_score)\n",
    "            # return if no flaw lines exist\n",
    "            if len(flaw_line_indices) == 0:\n",
    "                return \"NA\"\n",
    "            total_lines, num_of_flaw_lines, all_correctly_predicted_flaw_lines, min_clean_lines_inspected, max_clean_lines_inspected, all_correctly_localized_func, top_10_correct_idx, top_10_not_correct_idx \\\n",
    "            = \\\n",
    "            line_level_evaluation(all_lines_score=all_lines_score, flaw_line_indices=flaw_line_indices, top_k_loc=top_k_loc, top_k_constant=top_k_constant, true_positive_only=True, index=index)\n",
    "        elif reasoning_method == \"lig\":\n",
    "            ref_token_id, sep_token_id, cls_token_id = tokenizer.pad_token_id, tokenizer.sep_token_id, tokenizer.cls_token_id\n",
    "            ref_input_ids = create_ref_input_ids(input_ids, ref_token_id, sep_token_id, cls_token_id)\n",
    "            # send data to device\n",
    "            input_ids = input_ids.to(args.device)\n",
    "            labels = labels.to(args.device)\n",
    "            ref_input_ids = ref_input_ids.to(args.device)\n",
    "            lig = LayerIntegratedGradients(lig_forward, model.module.encoder.roberta.embeddings)\n",
    "            #print(input_ids.shape, ref_input_ids.shape)\n",
    "            attributions, delta = lig.attribute(inputs=input_ids,\n",
    "                                                baselines=ref_input_ids,\n",
    "                                                internal_batch_size=1,\n",
    "                                                return_convergence_delta=True)\n",
    "            score = predict(input_ids)\n",
    "            \n",
    "            pred_idx = torch.argmax(score).cpu().numpy()\n",
    "            pred_prob = score[pred_idx]\n",
    "            attributions_sum = summarize_attributions(attributions)        \n",
    "            attr_scores = attributions_sum.tolist()\n",
    "            # each token should have one score\n",
    "            assert len(all_tokens) == len(attr_scores)\n",
    "            # store tokens and attr scores together in a list of tuple [(token, attr_score)]\n",
    "            word_attr_scores = get_word_att_scores(all_tokens=all_tokens, att_scores=attr_scores)\n",
    "            # remove <s>, </s>, <unk>, <pad>\n",
    "            word_attr_scores = clean_word_attr_scores(word_attr_scores=word_attr_scores)\n",
    "            all_lines_score, flaw_line_indices = get_all_lines_score(word_attr_scores, verified_flaw_lines)\n",
    "            # return if no flaw lines exist\n",
    "            if len(flaw_line_indices) == 0:\n",
    "                return \"NA\"\n",
    "            total_lines, num_of_flaw_lines, all_correctly_predicted_flaw_lines, min_clean_lines_inspected, max_clean_lines_inspected, all_correctly_localized_func, top_10_correct_idx, top_10_not_correct_idx \\\n",
    "             = \\\n",
    "            line_level_evaluation(all_lines_score=all_lines_score, flaw_line_indices=flaw_line_indices, top_k_loc=top_k_loc, top_k_constant=top_k_constant, true_positive_only=True, index=index)\n",
    "        elif reasoning_method == \"deeplift\" or \\\n",
    "             reasoning_method == \"deeplift_shap\" or \\\n",
    "             reasoning_method == \"gradient_shap\" or \\\n",
    "             reasoning_method == \"saliency\":\n",
    "            # send data to device\n",
    "            input_ids = input_ids.to(args.device)\n",
    "            input_embed = model.module.encoder.roberta.embeddings(input_ids).to(args.device)\n",
    "            if reasoning_method == \"deeplift\":\n",
    "                #baselines = torch.randn(1, 512, 768, requires_grad=True).to(args.device)\n",
    "                baselines = torch.zeros(1, 512, 768, requires_grad=True).to(args.device)\n",
    "                reasoning_model = DeepLift(model)\n",
    "            elif reasoning_method == \"deeplift_shap\":\n",
    "                #baselines = torch.randn(16, 512, 768, requires_grad=True).to(args.device)\n",
    "                baselines = torch.zeros(16, 512, 768, requires_grad=True).to(args.device)\n",
    "                reasoning_model = DeepLiftShap(model)\n",
    "            elif reasoning_method == \"gradient_shap\":\n",
    "                #baselines = torch.randn(16, 512, 768, requires_grad=True).to(args.device)\n",
    "                baselines = torch.zeros(16, 512, 768, requires_grad=True).to(args.device)\n",
    "                reasoning_model = GradientShap(model)\n",
    "            elif reasoning_method == \"saliency\":\n",
    "                reasoning_model = Saliency(model)\n",
    "            # attributions -> [1, 512, 768]\n",
    "            if reasoning_method == \"saliency\":\n",
    "                attributions = reasoning_model.attribute(input_embed, target=1)\n",
    "            else:\n",
    "                attributions = reasoning_model.attribute(input_embed, baselines=baselines, target=1)\n",
    "            attributions_sum = summarize_attributions(attributions)        \n",
    "            attr_scores = attributions_sum.tolist()\n",
    "            # each token should have one score\n",
    "            assert len(all_tokens) == len(attr_scores)\n",
    "            # store tokens and attr scores together in a list of tuple [(token, attr_score)]\n",
    "            word_attr_scores = get_word_att_scores(all_tokens=all_tokens, att_scores=attr_scores)\n",
    "            # remove <s>, </s>, <unk>, <pad>\n",
    "            word_attr_scores = clean_word_attr_scores(word_attr_scores=word_attr_scores)\n",
    "            all_lines_score, flaw_line_indices = get_all_lines_score(word_attr_scores, verified_flaw_lines)\n",
    "            # return if no flaw lines exist\n",
    "            if len(flaw_line_indices) == 0:\n",
    "                return \"NA\"\n",
    "            total_lines, num_of_flaw_lines, all_correctly_predicted_flaw_lines, min_clean_lines_inspected, max_clean_lines_inspected, all_correctly_localized_func, top_10_correct_idx, top_10_not_correct_idx \\\n",
    "             = \\\n",
    "            line_level_evaluation(all_lines_score=all_lines_score, flaw_line_indices=flaw_line_indices, top_k_loc=top_k_loc, top_k_constant=top_k_constant, true_positive_only=True, index=index)        \n",
    "      \n",
    "        results = {\"total_lines\": total_lines,\n",
    "                    \"num_of_flaw_lines\": num_of_flaw_lines,\n",
    "                    \"all_correctly_predicted_flaw_lines\": all_correctly_predicted_flaw_lines,\n",
    "                    \"all_correctly_localized_function\": all_correctly_localized_func,\n",
    "                    \"min_clean_lines_inspected\": min_clean_lines_inspected,\n",
    "                    \"max_clean_lines_inspected\": max_clean_lines_inspected,\n",
    "                    \"top_10_correct_idx\": top_10_correct_idx,\n",
    "                    \"top_10_not_correct_idx\": top_10_not_correct_idx}\n",
    "        #print(results)\n",
    "        return results\n",
    "    else:\n",
    "        if write_invalid_data:\n",
    "            with open(\"../invalid_data/invalid_line_lev_data.txt\", \"a\") as f:\n",
    "                f.writelines(\"--- ALL TOKENS ---\")\n",
    "                f.writelines(\"\\n\")\n",
    "                alltok = ''.join(all_tokens)\n",
    "                alltok = alltok.split(\"Ċ\")\n",
    "                for tok in alltok:\n",
    "                    f.writelines(tok)\n",
    "                    f.writelines(\"\\n\")\n",
    "                f.writelines(\"--- FLAW ---\")\n",
    "                f.writelines(\"\\n\")\n",
    "                for i in range(len(flaw_tokens_encoded)):\n",
    "                    f.writelines(''.join(flaw_tokens_encoded[i]))\n",
    "                    f.writelines(\"\\n\")\n",
    "                f.writelines(\"\\n\")\n",
    "                f.writelines(\"\\n\")\n",
    "    # if no flaw line exist in the encoded input\n",
    "    return \"NA\"\n",
    "\n",
    "def line_level_localization(flaw_lines: str, tokenizer, model, mini_batch, original_func: str, args,\n",
    "                            top_k_loc: list, top_k_constant: list, reasoning_method: str, index: int):\n",
    "    \n",
    "    print(\"Reasoning Method \", reasoning_method)\n",
    "    print(\"Inside line level loc\", original_func)\n",
    "    # function for captum LIG.\n",
    "    def predict(input_ids):\n",
    "        return model(input_ids=input_ids)[0]\n",
    "\n",
    "    def lig_forward(input_ids):\n",
    "        logits = model(input_ids=input_ids)[0]\n",
    "        y_pred = 1 # for positive attribution, y_pred = 0 for negative attribution\n",
    "        pred_prob = logits[y_pred].unsqueeze(-1)\n",
    "        return pred_prob\n",
    "\n",
    "    flaw_line_seperator = \"/~/\"\n",
    "    \n",
    "    (input_ids, labels, lines) = mini_batch\n",
    "\n",
    "    # void GoBackCrossSite() \n",
    "    # staticint__net_initicmp_sk_init(structnet*net)    \n",
    "\n",
    "    ids = input_ids[0].detach().tolist()\n",
    "    all_tokens = tokenizer.convert_ids_to_tokens(ids)\n",
    "    all_tokens = [token.replace(\"Ġ\", \"\") for token in all_tokens]\n",
    "    all_tokens = [token.replace(\"ĉ\", \"Ċ\") for token in all_tokens]\n",
    "    original_lines = ''.join(all_tokens).split(\"Ċ\")\n",
    "\n",
    "    print(\"Original Lines,*********** \", original_lines)\n",
    "    new_original_lines = []\n",
    "    for line in original_lines:\n",
    "        if line != \"\":\n",
    "            new_original_lines.append(line)\n",
    "    print(\"New Original Lines, ********* \", len(new_original_lines),  new_original_lines)\n",
    "    print(\"Flaw Lines\", flaw_lines)\n",
    "\n",
    "    # flaw line verification\n",
    "    # get flaw tokens ground truth\n",
    "    flaw_lines = get_all_flaw_lines(flaw_lines=flaw_lines, flaw_line_seperator=flaw_line_seperator)\n",
    "    flaw_tokens_encoded = encode_all_lines(all_lines=flaw_lines, tokenizer=tokenizer)\n",
    "    verified_flaw_lines = []\n",
    "    for i in range(len(flaw_tokens_encoded)):\n",
    "        encoded_flaw = ''.join(flaw_tokens_encoded[i])\n",
    "        encoded_all = ''.join(all_tokens)\n",
    "        if encoded_flaw in encoded_all:\n",
    "            verified_flaw_lines.append(flaw_tokens_encoded[i])\n",
    "\n",
    "    if reasoning_method == \"attention\":\n",
    "        # attentions: a tuple with of one Tensor with 4D shape (batch_size, num_heads, sequence_length, sequence_length)\n",
    "        input_ids = input_ids.to(args.device)\n",
    "        model.eval()\n",
    "        model.to(args.device)\n",
    "        with torch.no_grad():\n",
    "            prob, attentions = model(input_ids=input_ids, output_attentions=True)\n",
    "        # take from tuple then take out mini-batch attention values\n",
    "        attentions = attentions[0][0]\n",
    "        attention = None\n",
    "        # go into the layer\n",
    "        for i in range(len(attentions)):\n",
    "            layer_attention = attentions[i]\n",
    "            # summerize the values of each token dot other tokens\n",
    "            layer_attention = sum(layer_attention)\n",
    "            if attention is None:\n",
    "                attention = layer_attention\n",
    "            else:\n",
    "                attention += layer_attention\n",
    "        # clean att score for <s> and </s>\n",
    "        attention = clean_special_token_values(attention, padding=True)\n",
    "        # attention should be 1D tensor with seq length representing each token's attention value\n",
    "        word_att_scores = get_word_att_scores(all_tokens=all_tokens, att_scores=attention)\n",
    "        #print(\"printing word attention scores ...\", word_att_scores)\n",
    "        all_lines_score, flaw_line_indices = get_all_lines_score(word_att_scores, verified_flaw_lines)\n",
    "        print(all_lines_score)\n",
    "        print(len(all_lines_score), len(original_lines))\n",
    "        all_lines_score_with_label = \\\n",
    "        line_level_evaluation(all_lines_score=all_lines_score, flaw_line_indices=flaw_line_indices, top_k_loc=top_k_loc, top_k_constant=top_k_constant, true_positive_only=False)\n",
    "    elif reasoning_method == \"lig\":\n",
    "        ref_token_id, sep_token_id, cls_token_id = tokenizer.pad_token_id, tokenizer.sep_token_id, tokenizer.cls_token_id\n",
    "        ref_input_ids = create_ref_input_ids(input_ids, ref_token_id, sep_token_id, cls_token_id)\n",
    "        # send data to device\n",
    "        input_ids = input_ids.to(args.device)\n",
    "        labels = labels.to(args.device)\n",
    "        ref_input_ids = ref_input_ids.to(args.device)\n",
    "\n",
    "        lig = LayerIntegratedGradients(lig_forward, model.module.encoder.roberta.embeddings)\n",
    "\n",
    "        attributions, delta = lig.attribute(inputs=input_ids,\n",
    "                                            baselines=ref_input_ids,\n",
    "                                            internal_batch_size=32,\n",
    "                                            return_convergence_delta=True)\n",
    "        score = predict(input_ids)\n",
    "        pred_idx = torch.argmax(score).cpu().numpy()\n",
    "        pred_prob = score[pred_idx]\n",
    "        attributions_sum = summarize_attributions(attributions)        \n",
    "        attr_scores = attributions_sum.tolist()\n",
    "        # each token should have one score\n",
    "        assert len(all_tokens) == len(attr_scores)\n",
    "        # store tokens and attr scores together in a list of tuple [(token, attr_score)]\n",
    "        word_attr_scores = get_word_att_scores(all_tokens=all_tokens, att_scores=attr_scores)\n",
    "        # remove <s>, </s>, <unk>, <pad>\n",
    "        word_attr_scores = clean_word_attr_scores(word_attr_scores=word_attr_scores)\n",
    "        all_lines_score, flaw_line_indices = get_all_lines_score(word_attr_scores, verified_flaw_lines)\n",
    "        all_lines_score_with_label = \\\n",
    "        line_level_evaluation(all_lines_score=all_lines_score, flaw_line_indices=flaw_line_indices, top_k_loc=top_k_loc, top_k_constant=top_k_constant, true_positive_only=False)\n",
    "    elif reasoning_method == \"deeplift\" or \\\n",
    "            reasoning_method == \"deeplift_shap\" or \\\n",
    "            reasoning_method == \"gradient_shap\" or \\\n",
    "            reasoning_method == \"saliency\":\n",
    "        # send data to device\n",
    "        input_ids = input_ids.to(args.device)\n",
    "        input_embed = model.module.encoder.roberta.embeddings(input_ids).to(args.device)\n",
    "        if reasoning_method == \"deeplift\":\n",
    "            #baselines = torch.randn(1, 512, 768, requires_grad=True).to(args.device)\n",
    "            baselines = torch.zeros(1, 512, 768, requires_grad=True).to(args.device)\n",
    "            reasoning_model = DeepLift(model)\n",
    "        elif reasoning_method == \"deeplift_shap\":\n",
    "            #baselines = torch.randn(16, 512, 768, requires_grad=True).to(args.device)\n",
    "            baselines = torch.zeros(16, 512, 768, requires_grad=True).to(args.device)\n",
    "            reasoning_model = DeepLiftShap(model)\n",
    "        elif reasoning_method == \"gradient_shap\":\n",
    "            #baselines = torch.randn(16, 512, 768, requires_grad=True).to(args.device)\n",
    "            baselines = torch.zeros(16, 512, 768, requires_grad=True).to(args.device)\n",
    "            reasoning_model = GradientShap(model)\n",
    "        elif reasoning_method == \"saliency\":\n",
    "            reasoning_model = Saliency(model)\n",
    "        # attributions -> [1, 512, 768]\n",
    "        if reasoning_method == \"saliency\":\n",
    "            attributions = reasoning_model.attribute(input_embed, target=1)\n",
    "        else:\n",
    "            attributions = reasoning_model.attribute(input_embed, baselines=baselines, target=1)\n",
    "        attributions_sum = summarize_attributions(attributions)        \n",
    "        attr_scores = attributions_sum.tolist()\n",
    "        # each token should have one score\n",
    "        assert len(all_tokens) == len(attr_scores)\n",
    "        # store tokens and attr scores together in a list of tuple [(token, attr_score)]\n",
    "        word_attr_scores = get_word_att_scores(all_tokens=all_tokens, att_scores=attr_scores)\n",
    "        # remove <s>, </s>, <unk>, <pad>\n",
    "        word_attr_scores = clean_word_attr_scores(word_attr_scores=word_attr_scores)\n",
    "        all_lines_score, flaw_line_indices = get_all_lines_score(word_attr_scores, verified_flaw_lines)\n",
    "\n",
    "        # Added by Nafis\n",
    "        print(all_lines_score)\n",
    "        print(len(all_lines_score), len(original_lines))\n",
    "\n",
    "        all_lines_score_with_label = \\\n",
    "        line_level_evaluation(all_lines_score=all_lines_score, flaw_line_indices=flaw_line_indices, top_k_loc=top_k_loc, top_k_constant=top_k_constant, true_positive_only=False)        \n",
    "        \n",
    "        \"\"\"\n",
    "        vis = viz.VisualizationDataRecord(\n",
    "                        attributions_sum,\n",
    "                        torch.max(torch.softmax(start_scores[0], dim=0)),\n",
    "                        torch.argmax(start_scores),\n",
    "                        torch.argmax(start_scores),\n",
    "                        str(ground_truth_start_ind),\n",
    "                        attributions_start_sum.sum(),       \n",
    "                        all_tokens,\n",
    "                        delta_start)\n",
    "        \"\"\"\n",
    "        \n",
    "    return all_lines_score_with_label\n",
    "\n",
    "def line_level_evaluation(all_lines_score: list, flaw_line_indices: list, top_k_loc: list, top_k_constant: list, true_positive_only: bool, index=None):\n",
    "    if true_positive_only:    \n",
    "        # line indices ranking based on attr values \n",
    "        ranking = sorted(range(len(all_lines_score)), key=lambda i: all_lines_score[i], reverse=True)\n",
    "        # total flaw lines\n",
    "        num_of_flaw_lines = len(flaw_line_indices)\n",
    "        # clean lines + flaw lines\n",
    "        total_lines = len(all_lines_score)\n",
    "        ### TopK% Recall ###\n",
    "        all_correctly_predicted_flaw_lines = []  \n",
    "        ### IFA ###\n",
    "        ifa = True\n",
    "        all_clean_lines_inspected = []\n",
    "        for top_k in top_k_loc:\n",
    "            correctly_predicted_flaw_lines = 0\n",
    "            for indice in flaw_line_indices:\n",
    "                # if within top-k\n",
    "                k = int(len(all_lines_score) * top_k)\n",
    "                # if detecting any flaw lines\n",
    "                if indice in ranking[: k]:\n",
    "                    correctly_predicted_flaw_lines += 1\n",
    "                if ifa:\n",
    "                    # calculate Initial False Alarm\n",
    "                    # IFA counts how many clean lines are inspected until the first vulnerable line is found when inspecting the lines ranked by the approaches.\n",
    "                    flaw_line_idx_in_ranking = ranking.index(indice)\n",
    "                    # e.g. flaw_line_idx_in_ranking = 3 will include 1 vulnerable line and 3 clean lines\n",
    "                    all_clean_lines_inspected.append(flaw_line_idx_in_ranking)  \n",
    "            # for IFA\n",
    "            min_clean_lines_inspected = min(all_clean_lines_inspected)\n",
    "            # for All Effort\n",
    "            max_clean_lines_inspected = max(all_clean_lines_inspected)\n",
    "            # only do IFA and All Effort once\n",
    "            ifa = False\n",
    "            # append result for one top-k value\n",
    "            all_correctly_predicted_flaw_lines.append(correctly_predicted_flaw_lines)\n",
    "        \n",
    "        ### Top10 Accuracy ###\n",
    "        all_correctly_localized_func = []\n",
    "        top_10_correct_idx = []\n",
    "        top_10_not_correct_idx = []\n",
    "        correctly_located = False\n",
    "        for k in top_k_constant:\n",
    "            for indice in flaw_line_indices:\n",
    "                # if detecting any flaw lines\n",
    "                if indice in ranking[: k]:\n",
    "                    \"\"\"\n",
    "                    # extract example for the paper\n",
    "                    if index == 2797:\n",
    "                        print(\"2797\")\n",
    "                        print(\"ground truth flaw line index: \", indice)\n",
    "                        print(\"ranked line\")\n",
    "                        print(ranking)\n",
    "                        print(\"original score\")\n",
    "                        print(all_lines_score)\n",
    "                    \"\"\"\n",
    "                    # append result for one top-k value\n",
    "                    all_correctly_localized_func.append(1)\n",
    "                    correctly_located = True\n",
    "                else:\n",
    "                    all_correctly_localized_func.append(0)\n",
    "            if correctly_located:\n",
    "                top_10_correct_idx.append(index)\n",
    "            else:\n",
    "                top_10_not_correct_idx.append(index)\n",
    "        return total_lines, num_of_flaw_lines, all_correctly_predicted_flaw_lines, min_clean_lines_inspected, max_clean_lines_inspected, all_correctly_localized_func, \\\n",
    "               top_10_correct_idx, top_10_not_correct_idx\n",
    "    else:\n",
    "        # all_lines_score_with_label: [[line score, line level label], [line score, line level label], ...]\n",
    "        all_lines_score_with_label = []\n",
    "        for i in range(len(all_lines_score)):\n",
    "            if i in flaw_line_indices:\n",
    "                all_lines_score_with_label.append([all_lines_score[i], 1])\n",
    "            else:\n",
    "                all_lines_score_with_label.append([all_lines_score[i], 0])\n",
    "        return all_lines_score_with_label\n",
    "    \n",
    "def clean_special_token_values(all_values, padding=False):\n",
    "    # special token in the beginning of the seq \n",
    "    all_values[0] = 0\n",
    "    if padding:\n",
    "        # get the last non-zero value which represents the att score for </s> token\n",
    "        idx = [index for index, item in enumerate(all_values) if item != 0][-1]\n",
    "        all_values[idx] = 0\n",
    "    else:\n",
    "        # special token in the end of the seq \n",
    "        all_values[-1] = 0\n",
    "    return all_values\n",
    "\n",
    "def clean_shap_tokens(all_tokens):\n",
    "    for i in range(len(all_tokens)):\n",
    "        all_tokens[i] = all_tokens[i].replace('Ġ', '')\n",
    "    return all_tokens\n",
    "\n",
    "def get_all_lines_score(word_att_scores: list, verified_flaw_lines: list):\n",
    "    verified_flaw_lines = [''.join(l) for l in verified_flaw_lines]\n",
    "    # word_att_scores -> [[token, att_value], [token, att_value], ...]\n",
    "    separator = [\"Ċ\", \" Ċ\", \"ĊĊ\", \" ĊĊ\"]\n",
    "    # to return\n",
    "    all_lines_score = []\n",
    "    score_sum = 0\n",
    "    line_idx = 0\n",
    "    flaw_line_indices = []\n",
    "    line = \"\"\n",
    "    for i in range(len(word_att_scores)):\n",
    "        # summerize if meet line separator or the last token\n",
    "        if ((word_att_scores[i][0] in separator) or (i == (len(word_att_scores) - 1))) and score_sum != 0:\n",
    "            score_sum += word_att_scores[i][1]\n",
    "            all_lines_score.append(score_sum)\n",
    "            is_flaw_line = False\n",
    "            for l in verified_flaw_lines:\n",
    "                if l == line:\n",
    "                    is_flaw_line = True\n",
    "            if is_flaw_line:\n",
    "                flaw_line_indices.append(line_idx)\n",
    "            line = \"\"\n",
    "            score_sum = 0\n",
    "            line_idx += 1\n",
    "        # else accumulate score\n",
    "        elif word_att_scores[i][0] not in separator:\n",
    "            line += word_att_scores[i][0]\n",
    "            score_sum += word_att_scores[i][1]\n",
    "    return all_lines_score, flaw_line_indices\n",
    "\n",
    "def get_all_flaw_lines(flaw_lines: str, flaw_line_seperator: str) -> list:\n",
    "    if isinstance(flaw_lines, str):\n",
    "        flaw_lines = flaw_lines.strip(flaw_line_seperator)\n",
    "        flaw_lines = flaw_lines.split(flaw_line_seperator)\n",
    "        flaw_lines = [line.strip() for line in flaw_lines]\n",
    "    else:\n",
    "        flaw_lines = []\n",
    "    return flaw_lines\n",
    "\n",
    "def encode_all_lines(all_lines: list, tokenizer) -> list:\n",
    "    encoded = []\n",
    "    for line in all_lines:\n",
    "        encoded.append(encode_one_line(line=line, tokenizer=tokenizer))\n",
    "    return encoded\n",
    "\n",
    "def get_word_att_scores(all_tokens: list, att_scores: list) -> list:\n",
    "    word_att_scores = []\n",
    "    for i in range(len(all_tokens)):\n",
    "        token, att_score = all_tokens[i], att_scores[i]\n",
    "        word_att_scores.append([token, att_score])\n",
    "    return word_att_scores\n",
    "\n",
    "def clean_word_attr_scores(word_attr_scores: list) -> list:\n",
    "    to_be_cleaned = ['<s>', '</s>', '<unk>', '<pad>']\n",
    "    cleaned = []\n",
    "    for word_attr_score in word_attr_scores:\n",
    "        if word_attr_score[0] not in to_be_cleaned:\n",
    "            cleaned.append(word_attr_score)\n",
    "    return cleaned\n",
    "    \n",
    "def encode_one_line(line, tokenizer):\n",
    "    # add \"@ \" at the beginning to ensure the encoding consistency, i.e., previous -> previous, not previous > pre + vious\n",
    "    code_tokens = tokenizer.tokenize(\"@ \" + line)\n",
    "    return [token.replace(\"Ġ\", \"\") for token in code_tokens if token != \"@\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddd223a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Argument \", args.reasoning_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766098d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "set_seed(args)\n",
    "config = RobertaConfig.from_pretrained(args.config_name if args.config_name else args.model_name_or_path)\n",
    "config.num_labels = 1\n",
    "config.num_attention_heads = args.num_attention_heads\n",
    "if args.use_word_level_tokenizer:\n",
    "    print('using wordlevel tokenizer!')\n",
    "    tokenizer = Tokenizer.from_file('./word_level_tokenizer/wordlevel.json')\n",
    "elif args.use_non_pretrained_tokenizer:\n",
    "    tokenizer = RobertaTokenizer(vocab_file=\"bpe_tokenizer/bpe_tokenizer-vocab.json\",\n",
    "                                 merges_file=\"bpe_tokenizer/bpe_tokenizer-merges.txt\")\n",
    "else:\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(args.tokenizer_name)\n",
    "if args.use_non_pretrained_model:\n",
    "    model = RobertaForSequenceClassification(config=config)        \n",
    "else:\n",
    "    model = RobertaForSequenceClassification.from_pretrained(args.model_name_or_path, config=config, ignore_mismatched_sizes=True)    \n",
    "\n",
    "#print(\"printing original model ...\", model)\n",
    "\n",
    "model = Model(model, config, tokenizer, args)\n",
    "#print(\"printing final model ...\", model.encoder.roberta)\n",
    "logger.info(\"Training/evaluation parameters %s\", args)\n",
    "# Training\n",
    "if args.do_train:\n",
    "    train_dataset = TextDataset(tokenizer, args, file_type='train')\n",
    "    eval_dataset = TextDataset(tokenizer, args, file_type='eval')\n",
    "    train(args, train_dataset, model, tokenizer, eval_dataset)\n",
    "# Evaluation\n",
    "results = {}\n",
    "\"\"\"\n",
    "if args.do_eval:\n",
    "    checkpoint_prefix = f'checkpoint-best-IoU/{args.model_name}'\n",
    "    output_dir = os.path.join(args.output_dir, '{}'.format(checkpoint_prefix))  \n",
    "    model.load_state_dict(torch.load(output_dir))\n",
    "    model.to(args.device)\n",
    "    result=evaluate(args, model, tokenizer)   \n",
    "\"\"\"\n",
    "#if args.do_test:\n",
    "\n",
    "\n",
    "checkpoint_prefix = f'checkpoint-best-IoU/{args.model_name}'\n",
    "output_dir = os.path.join(args.output_dir, '{}'.format(checkpoint_prefix))  \n",
    "model.load_state_dict(torch.load(output_dir, map_location=args.device))\n",
    "model.to(args.device)\n",
    "test_dataset = TextDataset(tokenizer, args, file_type='test')\n",
    "\n",
    "print(\"length of test dataset\", len(test_dataset))\n",
    "test(args, model, tokenizer, test_dataset, best_threshold=0.5)\n",
    "#return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfeac259",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6619cd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e555bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-base')\n",
    "model = T5ForConditionalGeneration.from_pretrained('Salesforce/codet5-base')\n",
    "\n",
    "text = \"def greet(user): print(f'hello <extra_id_0>!')\"\n",
    "input_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# simply generate a single sequence\n",
    "generated_ids = model.generate(input_ids, max_length=8)\n",
    "print(tokenizer.decode(generated_ids[0], skip_special_tokens=True))\n",
    "# this prints \"{user.username}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07157e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "void GoBackCrossSite() {\n",
    "NavigationEntry* entry = contents()->controller().GetEntryAtOffset(-1);\n",
    "ASSERT_TRUE(entry);\n",
    "contents()->controller().GoBack();\n",
    "\n",
    "    // The navigation should commit in the pending RVH.\n",
    "    contents()->TestDidNavigate(\n",
    "        contents()->pending_rvh(), entry->page_id(), GURL(entry->url()),\n",
    "        content::PAGE_TRANSITION_TYPED);\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
